# -*- coding: utf-8 -*-
"""Dreamer Repurposed

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XxmwQAC0eKrElmjIT41hI1QB7sTmubxR

# DreamerV2 Deconstruction

OG: https://github.com/danijar/dreamerv2

## Initial Git Pull & Import
"""

!pip install gym_minigrid gym tensorflow_probability ruamel.yaml 'gym[atari]' dm_control

!git clone https://github.com/danijar/dreamerv2.git
!mv dreamerv2/dreamerv2 dreamer_v2 && rm -r dreamerv2 && mv dreamer_v2 dreamerv2
!rm -r common
!cp -r dreamerv2/common common
# !cp dreamerv2/configs.yaml configs.yaml

import os

## Some special magic to make all of the common files' asserts mere "suggestions"
## Important because command-line argument checker unnecessarily fails assert b/c of colab 
dir = 'common'
for fname in os.listdir(dir):
    if fname.endswith(".py"):
        fpath = os.path.join(dir, fname)
        print(f"Handling {fpath}")
        data = ""
        with open(fpath, 'r') as fin: 
            for ln, line in enumerate(fin):
                if ln == 0: 
                    line = 'def try_assert(loc, cond, msg="No Message"):\n\tif not cond: print(f" > Assertion Failed At ({loc}): {msg}")\n\n''' + line
                elif 'assert' in line: 
                    print(f' - {line}')
                    line = line.replace('assert', f'try_assert("{fpath}:{ln}", ').replace('\n', ')\n')
                    # line = line.replace('assert', '# assert')
                    print(f' + {line}')
                data += line
        with open(fpath, 'w') as fout: 
            fout.write(data)

"""## Import Pool"""

import collections
import functools
import logging
import os
import pathlib
import re
import sys
import warnings

try:
  import rich.traceback
  rich.traceback.install()
except ImportError:
  pass

import tensorflow as tf
from tensorflow.keras import mixed_precision as prec

import common

from common import Config
from common import GymWrapper
from common import RenderImage
from common import TerminalOutput
from common import JSONLOutput
from common import TensorBoardOutput

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
logging.getLogger().setLevel('ERROR')
warnings.filterwarnings('ignore', '.*box bound precision lowered.*')

# sys.path.append(str(pathlib.Path(__file__).parent))
# sys.path.append(str(pathlib.Path(__file__).parent.parent))

import numpy as np
import ruamel.yaml as yaml

"""## YAML Modification"""

yaml_dict = {    ## Just to show an example of how to update the yaml entries.
    'seed' : "0",
    'expl_behavior' : "greedy"
}

with open('dreamerv2/configs.yaml', 'r') as fin: 
    with open('configs.yaml', 'w') as fout:
        for line in fin: 
            try:    
                key, value = line.split(': ')
                if key in yaml_dict.keys():
                    line = line.replace(value, yaml_dict[key])
            except: 
                pass
            fout.write(line)

"""## agent.py"""

class Agent(common.Module):

  def __init__(self, config, obs_space, act_space, step):
    self.config = config
    self.obs_space = obs_space
    self.act_space = act_space['action']
    self.step = step
    self.tfstep = tf.Variable(int(self.step), tf.int64)
    self.wm = WorldModel(config, obs_space, self.tfstep)
    self._task_behavior = ActorCritic(config, self.act_space, self.tfstep)
    if config.expl_behavior == 'greedy':
      self._expl_behavior = self._task_behavior
    else:
      self._expl_behavior = getattr(expl, config.expl_behavior)(
          self.config, self.act_space, self.wm, self.tfstep,
          lambda seq: self.wm.heads['reward'](seq['feat']).mode())

  @tf.function
  def policy(self, obs, state=None, mode='train'):
    obs = tf.nest.map_structure(tf.tensor, obs)
    tf.py_function(lambda: self.tfstep.assign(
        int(self.step), read_value=False), [], [])
    if state is None:
      latent = self.wm.rssm.initial(len(obs['reward']))
      action = tf.zeros((len(obs['reward']),) + self.act_space.shape)
      state = latent, action
    latent, action = state
    embed = self.wm.encoder(self.wm.preprocess(obs))
    sample = (mode == 'train') or not self.config.eval_state_mean
    latent, _ = self.wm.rssm.obs_step(
        latent, action, embed, obs['is_first'], sample)
    feat = self.wm.rssm.get_feat(latent)
    if mode == 'eval':
      actor = self._task_behavior.actor(feat)
      action = actor.mode()
      noise = self.config.eval_noise
    elif mode == 'explore':
      actor = self._expl_behavior.actor(feat)
      action = actor.sample()
      noise = self.config.expl_noise
    elif mode == 'train':
      actor = self._task_behavior.actor(feat)
      action = actor.sample()
      noise = self.config.expl_noise
    action = common.action_noise(action, noise, self.act_space)
    outputs = {'action': action}
    state = (latent, action)
    return outputs, state

  @tf.function
  def train(self, data, state=None):
    metrics = {}
    state, outputs, mets = self.wm.train(data, state)
    metrics.update(mets)
    start = outputs['post']
    reward = lambda seq: self.wm.heads['reward'](seq['feat']).mode()
    metrics.update(self._task_behavior.train(
        self.wm, start, data['is_terminal'], reward))
    if self.config.expl_behavior != 'greedy':
      mets = self._expl_behavior.train(start, outputs, data)[-1]
      metrics.update({'expl_' + key: value for key, value in mets.items()})
    return state, metrics

  @tf.function
  def report(self, data):
    report = {}
    data = self.wm.preprocess(data)
    for key in self.wm.heads['decoder'].cnn_keys:
      name = key.replace('/', '_')
      report[f'openl_{name}'] = self.wm.video_pred(data, key)
    return report


class WorldModel(common.Module):

  def __init__(self, config, obs_space, tfstep):
    shapes = {k: tuple(v.shape) for k, v in obs_space.items()}
    self.config = config
    self.tfstep = tfstep
    self.rssm = common.EnsembleRSSM(**config.rssm)
    self.encoder = common.Encoder(shapes, **config.encoder)
    self.heads = {}
    self.heads['decoder'] = common.Decoder(shapes, **config.decoder)
    self.heads['reward'] = common.MLP([], **config.reward_head)
    if config.pred_discount:
      self.heads['discount'] = common.MLP([], **config.discount_head)
    for name in config.grad_heads:
      assert name in self.heads, name
    self.model_opt = common.Optimizer('model', **config.model_opt)

  def train(self, data, state=None):
    with tf.GradientTape() as model_tape:
      model_loss, state, outputs, metrics = self.loss(data, state)
    modules = [self.encoder, self.rssm, *self.heads.values()]
    metrics.update(self.model_opt(model_tape, model_loss, modules))
    return state, outputs, metrics

  def loss(self, data, state=None):
    data = self.preprocess(data)
    embed = self.encoder(data)
    post, prior = self.rssm.observe(
        embed, data['action'], data['is_first'], state)
    kl_loss, kl_value = self.rssm.kl_loss(post, prior, **self.config.kl)
    assert len(kl_loss.shape) == 0
    likes = {}
    losses = {'kl': kl_loss}
    feat = self.rssm.get_feat(post)
    for name, head in self.heads.items():
      grad_head = (name in self.config.grad_heads)
      inp = feat if grad_head else tf.stop_gradient(feat)
      out = head(inp)
      dists = out if isinstance(out, dict) else {name: out}
      for key, dist in dists.items():
        like = tf.cast(dist.log_prob(data[key]), tf.float32)
        likes[key] = like
        losses[key] = -like.mean()
    model_loss = sum(
        self.config.loss_scales.get(k, 1.0) * v for k, v in losses.items())
    outs = dict(
        embed=embed, feat=feat, post=post,
        prior=prior, likes=likes, kl=kl_value)
    metrics = {f'{name}_loss': value for name, value in losses.items()}
    metrics['model_kl'] = kl_value.mean()
    metrics['prior_ent'] = self.rssm.get_dist(prior).entropy().mean()
    metrics['post_ent'] = self.rssm.get_dist(post).entropy().mean()
    last_state = {k: v[:, -1] for k, v in post.items()}
    return model_loss, last_state, outs, metrics

  def imagine(self, policy, start, is_terminal, horizon):
    flatten = lambda x: x.reshape([-1] + list(x.shape[2:]))
    start = {k: flatten(v) for k, v in start.items()}
    start['feat'] = self.rssm.get_feat(start)
    start['action'] = tf.zeros_like(policy(start['feat']).mode())
    seq = {k: [v] for k, v in start.items()}
    for _ in range(horizon):
      action = policy(tf.stop_gradient(seq['feat'][-1])).sample()
      state = self.rssm.img_step({k: v[-1] for k, v in seq.items()}, action)
      feat = self.rssm.get_feat(state)
      for key, value in {**state, 'action': action, 'feat': feat}.items():
        seq[key].append(value)
    seq = {k: tf.stack(v, 0) for k, v in seq.items()}
    if 'discount' in self.heads:
      disc = self.heads['discount'](seq['feat']).mean()
      if is_terminal is not None:
        # Override discount prediction for the first step with the true
        # discount factor from the replay buffer.
        true_first = 1.0 - flatten(is_terminal).astype(disc.dtype)
        true_first *= self.config.discount
        disc = tf.concat([true_first[None], disc[1:]], 0)
    else:
      disc = self.config.discount * tf.ones(seq['feat'].shape[:-1])
    seq['discount'] = disc
    # Shift discount factors because they imply whether the following state
    # will be valid, not whether the current state is valid.
    seq['weight'] = tf.math.cumprod(
        tf.concat([tf.ones_like(disc[:1]), disc[:-1]], 0), 0)
    return seq

  @tf.function
  def preprocess(self, obs):
    dtype = prec.global_policy().compute_dtype
    obs = obs.copy()
    for key, value in obs.items():
      if key.startswith('log_'):
        continue
      if value.dtype == tf.int32:
        value = value.astype(dtype)
      if value.dtype == tf.uint8:
        value = value.astype(dtype) / 255.0 - 0.5
      obs[key] = value
    obs['reward'] = {
        'identity': tf.identity,
        'sign': tf.sign,
        'tanh': tf.tanh,
    }[self.config.clip_rewards](obs['reward'])
    obs['discount'] = 1.0 - obs['is_terminal'].astype(dtype)
    obs['discount'] *= self.config.discount
    return obs

  @tf.function
  def video_pred(self, data, key):
    decoder = self.heads['decoder']
    truth = data[key][:6] + 0.5
    embed = self.encoder(data)
    states, _ = self.rssm.observe(
        embed[:6, :5], data['action'][:6, :5], data['is_first'][:6, :5])
    recon = decoder(self.rssm.get_feat(states))[key].mode()[:6]
    init = {k: v[:, -1] for k, v in states.items()}
    prior = self.rssm.imagine(data['action'][:6, 5:], init)
    openl = decoder(self.rssm.get_feat(prior))[key].mode()
    model = tf.concat([recon[:, :5] + 0.5, openl + 0.5], 1)
    error = (model - truth + 1) / 2
    video = tf.concat([truth, model, error], 2)
    B, T, H, W, C = video.shape
    return video.transpose((1, 2, 0, 3, 4)).reshape((T, H, B * W, C))


class ActorCritic(common.Module):

  def __init__(self, config, act_space, tfstep):
    self.config = config
    self.act_space = act_space
    self.tfstep = tfstep
    discrete = hasattr(act_space, 'n')
    if self.config.actor.dist == 'auto':
      self.config = self.config.update({
          'actor.dist': 'onehot' if discrete else 'trunc_normal'})
    if self.config.actor_grad == 'auto':
      self.config = self.config.update({
          'actor_grad': 'reinforce' if discrete else 'dynamics'})
    self.actor = common.MLP(act_space.shape[0], **self.config.actor)
    self.critic = common.MLP([], **self.config.critic)
    if self.config.slow_target:
      self._target_critic = common.MLP([], **self.config.critic)
      self._updates = tf.Variable(0, tf.int64)
    else:
      self._target_critic = self.critic
    self.actor_opt = common.Optimizer('actor', **self.config.actor_opt)
    self.critic_opt = common.Optimizer('critic', **self.config.critic_opt)
    self.rewnorm = common.StreamNorm(**self.config.reward_norm)

  def train(self, world_model, start, is_terminal, reward_fn):
    metrics = {}
    hor = self.config.imag_horizon
    # The weights are is_terminal flags for the imagination start states.
    # Technically, they should multiply the losses from the second trajectory
    # step onwards, which is the first imagined step. However, we are not
    # training the action that led into the first step anyway, so we can use
    # them to scale the whole sequence.
    with tf.GradientTape() as actor_tape:
      seq = world_model.imagine(self.actor, start, is_terminal, hor)
      reward = reward_fn(seq)
      seq['reward'], mets1 = self.rewnorm(reward)
      mets1 = {f'reward_{k}': v for k, v in mets1.items()}
      target, mets2 = self.target(seq)
      actor_loss, mets3 = self.actor_loss(seq, target)
    with tf.GradientTape() as critic_tape:
      critic_loss, mets4 = self.critic_loss(seq, target)
    metrics.update(self.actor_opt(actor_tape, actor_loss, self.actor))
    metrics.update(self.critic_opt(critic_tape, critic_loss, self.critic))
    metrics.update(**mets1, **mets2, **mets3, **mets4)
    self.update_slow_target()  # Variables exist after first forward pass.
    return metrics

  def actor_loss(self, seq, target):
    # Actions:      0   [a1]  [a2]   a3
    #                  ^  |  ^  |  ^  |
    #                 /   v /   v /   v
    # States:     [z0]->[z1]-> z2 -> z3
    # Targets:     t0   [t1]  [t2]
    # Baselines:  [v0]  [v1]   v2    v3
    # Entropies:        [e1]  [e2]
    # Weights:    [ 1]  [w1]   w2    w3
    # Loss:              l1    l2
    metrics = {}
    # Two states are lost at the end of the trajectory, one for the boostrap
    # value prediction and one because the corresponding action does not lead
    # anywhere anymore. One target is lost at the start of the trajectory
    # because the initial state comes from the replay buffer.
    policy = self.actor(tf.stop_gradient(seq['feat'][:-2]))
    if self.config.actor_grad == 'dynamics':
      objective = target[1:]
    elif self.config.actor_grad == 'reinforce':
      baseline = self._target_critic(seq['feat'][:-2]).mode()
      advantage = tf.stop_gradient(target[1:] - baseline)
      objective = policy.log_prob(seq['action'][1:-1]) * advantage
    elif self.config.actor_grad == 'both':
      baseline = self._target_critic(seq['feat'][:-2]).mode()
      advantage = tf.stop_gradient(target[1:] - baseline)
      objective = policy.log_prob(seq['action'][1:-1]) * advantage
      mix = common.schedule(self.config.actor_grad_mix, self.tfstep)
      objective = mix * target[1:] + (1 - mix) * objective
      metrics['actor_grad_mix'] = mix
    else:
      raise NotImplementedError(self.config.actor_grad)
    ent = policy.entropy()
    ent_scale = common.schedule(self.config.actor_ent, self.tfstep)
    objective += ent_scale * ent
    weight = tf.stop_gradient(seq['weight'])
    actor_loss = -(weight[:-2] * objective).mean()
    metrics['actor_ent'] = ent.mean()
    metrics['actor_ent_scale'] = ent_scale
    return actor_loss, metrics

  def critic_loss(self, seq, target):
    # States:     [z0]  [z1]  [z2]   z3
    # Rewards:    [r0]  [r1]  [r2]   r3
    # Values:     [v0]  [v1]  [v2]   v3
    # Weights:    [ 1]  [w1]  [w2]   w3
    # Targets:    [t0]  [t1]  [t2]
    # Loss:        l0    l1    l2
    dist = self.critic(seq['feat'][:-1])
    target = tf.stop_gradient(target)
    weight = tf.stop_gradient(seq['weight'])
    critic_loss = -(dist.log_prob(target) * weight[:-1]).mean()
    metrics = {'critic': dist.mode().mean()}
    return critic_loss, metrics

  def target(self, seq):
    # States:     [z0]  [z1]  [z2]  [z3]
    # Rewards:    [r0]  [r1]  [r2]   r3
    # Values:     [v0]  [v1]  [v2]  [v3]
    # Discount:   [d0]  [d1]  [d2]   d3
    # Targets:     t0    t1    t2
    reward = tf.cast(seq['reward'], tf.float32)
    disc = tf.cast(seq['discount'], tf.float32)
    value = self._target_critic(seq['feat']).mode()
    # Skipping last time step because it is used for bootstrapping.
    target = common.lambda_return(
        reward[:-1], value[:-1], disc[:-1],
        bootstrap=value[-1],
        lambda_=self.config.discount_lambda,
        axis=0)
    metrics = {}
    metrics['critic_slow'] = value.mean()
    metrics['critic_target'] = target.mean()
    return target, metrics

  def update_slow_target(self):
    if self.config.slow_target:
      if self._updates % self.config.slow_target_update == 0:
        mix = 1.0 if self._updates == 0 else float(
            self.config.slow_target_fraction)
        for s, d in zip(self.critic.variables, self._target_critic.variables):
          d.assign(mix * s + (1 - mix) * d)
      self._updates.assign_add(1)

"""## api.py"""

def train(env, config, outputs=None):

  logdir = pathlib.Path(config.logdir).expanduser()
  logdir.mkdir(parents=True, exist_ok=True)
  config.save(logdir / 'config.yaml')
  print(config, '\n')
  print('Logdir', logdir)

  outputs = outputs or [
      common.TerminalOutput(),
      common.JSONLOutput(config.logdir),
      common.TensorBoardOutput(config.logdir),
  ]
  replay = common.Replay(logdir / 'train_episodes', **config.replay)
  step = common.Counter(replay.stats['total_steps'])
  logger = common.Logger(step, outputs, multiplier=config.action_repeat)
  metrics = collections.defaultdict(list)

  should_train = common.Every(config.train_every)
  should_log = common.Every(config.log_every)
  should_video = common.Every(config.log_every)
  should_expl = common.Until(config.expl_until)

  def per_episode(ep):
    length = len(ep['reward']) - 1
    score = float(ep['reward'].astype(np.float64).sum())
    print(f'Episode has {length} steps and return {score:.1f}.')
    logger.scalar('return', score)
    logger.scalar('length', length)
    for key, value in ep.items():
      if re.match(config.log_keys_sum, key):
        logger.scalar(f'sum_{key}', ep[key].sum())
      if re.match(config.log_keys_mean, key):
        logger.scalar(f'mean_{key}', ep[key].mean())
      if re.match(config.log_keys_max, key):
        logger.scalar(f'max_{key}', ep[key].max(0).mean())
    if should_video(step):
      for key in config.log_keys_video:
        logger.video(f'policy_{key}', ep[key])
    logger.add(replay.stats)
    logger.write()

  env = common.GymWrapper(env)
  env = common.ResizeImage(env)
  if hasattr(env.act_space['action'], 'n'):
    env = common.OneHotAction(env)
  else:
    env = common.NormalizeAction(env)
  env = common.TimeLimit(env, config.time_limit)

  driver = common.Driver([env])
  driver.on_episode(per_episode)
  driver.on_step(lambda tran, worker: step.increment())
  driver.on_step(replay.add_step)
  driver.on_reset(replay.add_step)

  prefill = max(0, config.prefill - replay.stats['total_steps'])
  if prefill:
    print(f'Prefill dataset ({prefill} steps).')
    random_agent = common.RandomAgent(env.act_space)
    driver(random_agent, steps=prefill, episodes=1)
    driver.reset()

  print('Create agent')
  # agnt = agent_.Agent(config, env.obs_space, env.act_space, step)
  agnt = Agent(config, env.obs_space, env.act_space, step)
  dataset = iter(replay.dataset(**config.dataset))
  train_agent = common.CarryOverState(agnt.train)
  train_agent(next(dataset))
  if (logdir / 'variables.pkl').exists():
    agnt.load(logdir / 'variables.pkl')
  else:
    print('Pretrain agent')
    for _ in range(config.pretrain):
      train_agent(next(dataset))
  policy = lambda *args: agnt.policy(
      *args, mode='explore' if should_expl(step) else 'train')

  def train_step(tran, worker):
    if should_train(step):
      for _ in range(config.train_steps):
        mets = train_agent(next(dataset))
        [metrics[key].append(value) for key, value in mets.items()]
    if should_log(step):
      for name, values in metrics.items():
        logger.scalar(name, np.array(values, np.float64).mean())
        metrics[name].clear()
      logger.add(agnt.report(next(dataset)))
      logger.write(fps=True)
  driver.on_step(train_step)

  while step < config.steps:
    logger.write()
    driver(policy, steps=config.eval_every)
    agnt.save(logdir / 'variables.pkl')

"""## expl.py"""

class Random(common.Module):

  def __init__(self, config, act_space, wm, tfstep, reward):
    self.config = config
    self.act_space = self.act_space

  def actor(self, feat):
    shape = feat.shape[:-1] + self.act_space.shape
    if self.config.actor.dist == 'onehot':
      return common.OneHotDist(tf.zeros(shape))
    else:
      dist = tfd.Uniform(-tf.ones(shape), tf.ones(shape))
      return tfd.Independent(dist, 1)

  def train(self, start, context, data):
    return None, {}


class Plan2Explore(common.Module):

  def __init__(self, config, act_space, wm, tfstep, reward):
    self.config = config
    self.reward = reward
    self.wm = wm
    # self.ac = agent_.ActorCritic(config, act_space, tfstep)
    self.ac = ActorCritic(config, act_space, tfstep)
    self.actor = self.ac.actor
    stoch_size = config.rssm.stoch
    if config.rssm.discrete:
      stoch_size *= config.rssm.discrete
    size = {
        'embed': 32 * config.encoder.cnn_depth,
        'stoch': stoch_size,
        'deter': config.rssm.deter,
        'feat': config.rssm.stoch + config.rssm.deter,
    }[self.config.disag_target]
    self._networks = [
        common.MLP(size, **config.expl_head)
        for _ in range(config.disag_models)]
    self.opt = common.Optimizer('expl', **config.expl_opt)
    self.extr_rewnorm = common.StreamNorm(**self.config.expl_reward_norm)
    self.intr_rewnorm = common.StreamNorm(**self.config.expl_reward_norm)

  def train(self, start, context, data):
    metrics = {}
    stoch = start['stoch']
    if self.config.rssm.discrete:
      stoch = tf.reshape(
          stoch, stoch.shape[:-2] + (stoch.shape[-2] * stoch.shape[-1]))
    target = {
        'embed': context['embed'],
        'stoch': stoch,
        'deter': start['deter'],
        'feat': context['feat'],
    }[self.config.disag_target]
    inputs = context['feat']
    if self.config.disag_action_cond:
      action = tf.cast(data['action'], inputs.dtype)
      inputs = tf.concat([inputs, action], -1)
    metrics.update(self._train_ensemble(inputs, target))
    metrics.update(self.ac.train(
        self.wm, start, data['is_terminal'], self._intr_reward))
    return None, metrics

  def _intr_reward(self, seq):
    inputs = seq['feat']
    if self.config.disag_action_cond:
      action = tf.cast(seq['action'], inputs.dtype)
      inputs = tf.concat([inputs, action], -1)
    preds = [head(inputs).mode() for head in self._networks]
    disag = tf.tensor(preds).std(0).mean(-1)
    if self.config.disag_log:
      disag = tf.math.log(disag)
    reward = self.config.expl_intr_scale * self.intr_rewnorm(disag)[0]
    if self.config.expl_extr_scale:
      reward += self.config.expl_extr_scale * self.extr_rewnorm(
          self.reward(seq))[0]
    return reward

  def _train_ensemble(self, inputs, targets):
    if self.config.disag_offset:
      targets = targets[:, self.config.disag_offset:]
      inputs = inputs[:, :-self.config.disag_offset]
    targets = tf.stop_gradient(targets)
    inputs = tf.stop_gradient(inputs)
    with tf.GradientTape() as tape:
      preds = [head(inputs) for head in self._networks]
      loss = -sum([pred.log_prob(targets).mean() for pred in preds])
    metrics = self.opt(tape, loss, self._networks)
    return metrics


class ModelLoss(common.Module):

  def __init__(self, config, act_space, wm, tfstep, reward):
    self.config = config
    self.reward = reward
    self.wm = wm
    # self.ac = agent_.ActorCritic(config, act_space, tfstep)
    self.ac = ActorCritic(config, act_space, tfstep)
    self.actor = self.ac.actor
    self.head = common.MLP([], **self.config.expl_head)
    self.opt = common.Optimizer('expl', **self.config.expl_opt)

  def train(self, start, context, data):
    metrics = {}
    target = tf.cast(context[self.config.expl_model_loss], tf.float32)
    with tf.GradientTape() as tape:
      loss = -self.head(context['feat']).log_prob(target).mean()
    metrics.update(self.opt(tape, loss, self.head))
    metrics.update(self.ac.train(
        self.wm, start, data['is_terminal'], self._intr_reward))
    return None, metrics

  def _intr_reward(self, seq):
    reward = self.config.expl_intr_scale * self.head(seq['feat']).mode()
    if self.config.expl_extr_scale:
      reward += self.config.expl_extr_scale * self.reward(seq)
    return reward

"""## train.py [probably useless, maybe delete?]"""

# def main():

#   configs = yaml.safe_load((pathlib.Path(sys.argv[0]).parent / 'configs.yaml').read_text())
#   parsed, remaining = common.Flags(configs=['defaults']).parse(known_only=True)
#   config = common.Config(configs['defaults'])
#   for name in parsed.configs:
#     config = config.update(configs[name])
#   config = common.Flags(config).parse(remaining)

#   logdir = pathlib.Path(config.logdir).expanduser()
#   logdir.mkdir(parents=True, exist_ok=True)
#   config.save(logdir / 'config.yaml')
#   print(config, '\n')
#   print('Logdir', logdir)

#   ########################################################################
#   ## Just make sure GPUs are in working order
#   import tensorflow as tf
#   tf.config.experimental_run_functions_eagerly(not config.jit)
#   message = 'No GPU found. To actually train on CPU remove this assert.'
#   assert tf.config.experimental.list_physical_devices('GPU'), message
#   for gpu in tf.config.experimental.list_physical_devices('GPU'):
#     tf.config.experimental.set_memory_growth(gpu, True)
#   assert config.precision in (16, 32), config.precision
#   if config.precision == 16:
#     from tensorflow.keras.mixed_precision import experimental as prec
#     prec.set_policy(prec.Policy('mixed_float16'))
#   ########################################################################

#   train_replay = common.Replay(logdir / 'train_episodes', **config.replay)
#   eval_replay = common.Replay(logdir / 'eval_episodes', **dict(
#       capacity  = config.replay.capacity // 10,
#       minlen    = config.dataset.length,
#       maxlen    = config.dataset.length
#   ))
#   step = common.Counter(train_replay.stats['total_steps'])
#   outputs = [
#       common.TerminalOutput(),
#       common.JSONLOutput(logdir),
#       common.TensorBoardOutput(logdir),
#   ]
#   logger = common.Logger(step, outputs, multiplier=config.action_repeat)
#   metrics = collections.defaultdict(list)

#   should_train = common.Every(config.train_every)
#   should_log = common.Every(config.log_every)
#   should_video_train = common.Every(config.eval_every)
#   should_video_eval = common.Every(config.eval_every)
#   should_expl = common.Until(config.expl_until)

#   def make_env(mode):
#     suite, task = config.task.split('_', 1)
#     if suite == 'dmc':
#       env = common.DMC(task, config.action_repeat,   config.render_size,  config.dmc_camera)
#       env = common.NormalizeAction(env)
#     elif suite == 'atari':
#       env = common.Atari(task, config.action_repeat, config.render_size,  config.atari_grayscale)
#       env = common.OneHotAction(env)
#     elif suite == 'crafter':
#       assert config.action_repeat == 1
#       outdir = logdir / 'crafter' if mode == 'train' else None
#       reward = bool(['noreward', 'reward'].index(task)) or mode == 'eval'
#       env = common.Crafter(outdir, reward)
#       env = common.OneHotAction(env)
#     else:
#       raise NotImplementedError(suite)
#     env = common.TimeLimit(env, config.time_limit)
#     return env

#   def per_episode(ep, mode):
#     length = len(ep['reward']) - 1
#     score = float(ep['reward'].astype(np.float64).sum())
#     print(f'{mode.title()} episode has {length} steps and return {score:.1f}.')
#     logger.scalar(f'{mode}_return', score)
#     logger.scalar(f'{mode}_length', length)
#     for key, value in ep.items():
#       if re.match(config.log_keys_sum,  key):   logger.scalar(f'sum_{mode}_{key}',  ep[key].sum())
#       if re.match(config.log_keys_mean, key):   logger.scalar(f'mean_{mode}_{key}', ep[key].mean())
#       if re.match(config.log_keys_max,  key):   logger.scalar(f'max_{mode}_{key}',  ep[key].max(0).mean())
#     should = {'train': should_video_train, 'eval': should_video_eval}[mode]
#     if should(step):
#       for key in config.log_keys_video:
#         logger.video(f'{mode}_policy_{key}', ep[key])
#     replay = dict(train=train_replay, eval=eval_replay)[mode]
#     logger.add(replay.stats, prefix=mode)
#     logger.write()

#   print('Create envs.')
#   num_eval_envs = min(config.envs, config.eval_eps)
#   if config.envs_parallel == 'none':
#     train_envs  = [make_env('train')  for _ in range(config.envs)]
#     eval_envs   = [make_env('eval')   for _ in range(num_eval_envs)]
#   else:
#     make_async_env = lambda mode: common.Async(functools.partial(make_env, mode), config.envs_parallel)
#     train_envs  = [make_async_env('train')  for _ in range(config.envs)]
#     eval_envs   = [make_async_env('eval')   for _ in range(num_eval_envs)]
#   act_space = train_envs[0].act_space
#   obs_space = train_envs[0].obs_space
#   train_driver = common.Driver(train_envs)
#   train_driver.on_episode(lambda ep: per_episode(ep, mode='train'))
#   train_driver.on_step(lambda tran, worker: step.increment())
#   train_driver.on_step(train_replay.add_step)
#   train_driver.on_reset(train_replay.add_step)
#   eval_driver = common.Driver(eval_envs)
#   eval_driver.on_episode(lambda ep: per_episode(ep, mode='eval'))
#   eval_driver.on_episode(eval_replay.add_episode)

#   prefill = max(0, config.prefill - train_replay.stats['total_steps'])
#   if prefill:
#     print(f'Prefill dataset ({prefill} steps).')
#     random_agent = common.RandomAgent(act_space)
#     train_driver(random_agent, steps=prefill, episodes=1)
#     eval_driver(random_agent, episodes=1)
#     train_driver.reset()
#     eval_driver.reset()

#   print('Create agent')
#   train_dataset = iter(train_replay.dataset(**config.dataset))
#   report_dataset = iter(train_replay.dataset(**config.dataset))
#   eval_dataset = iter(eval_replay.dataset(**config.dataset))
#   # agnt = agent_.Agent(config, obs_space, act_space, step)
#   agnt = Agent(config, obs_space, act_space, step)
#   train_agent = common.CarryOverState(agnt.train)
#   train_agent(next(train_dataset))
  
#   if (logdir / 'variables.pkl').exists():
#     agnt.load(logdir / 'variables.pkl')
#   else:
#     print('Pretrain agent')
#     for _ in range(config.pretrain):
#       train_agent(next(train_dataset))
  
#   train_policy  = lambda *args: agnt.policy(*args, mode='explore' if should_expl(step) else 'train')
#   eval_policy   = lambda *args: agnt.policy(*args, mode='eval')

#   def train_step(tran, worker):
#     if should_train(step):
#       for _ in range(config.train_steps):
#         mets = train_agent(next(train_dataset))
#         [metrics[key].append(value) for key, value in mets.items()]
#     if should_log(step):
#       for name, values in metrics.items():
#         logger.scalar(name, np.array(values, np.float64).mean())
#         metrics[name].clear()
#       logger.add(agnt.report(next(report_dataset)), prefix='train')
#       logger.write(fps=True)
#   train_driver.on_step(train_step)

#   ## 
#   while step < config.steps:
#     logger.write()
#     print('Start evaluation.')
#     logger.add(agnt.report(next(eval_dataset)), prefix='eval')
#     eval_driver(eval_policy, episodes=config.eval_eps)
#     print('Start training.')
#     train_driver(train_policy, steps=config.eval_every)
#     agnt.save(logdir / 'variables.pkl')
  
#   ## Close all of the environments
#   for env in train_envs + eval_envs:
#     try:                env.close()
#     except Exception:   pass

"""## Runner"""

# configs = yaml.safe_load((pathlib.Path(__file__).parent / 'configs.yaml').read_text())
# defaults = common.Config(configs.pop('defaults'))

configs = yaml.safe_load(open('configs.yaml').read())
defaults = common.Config(configs.pop('defaults'))

config = defaults.update({
    'logdir': '~/logdir/minigrid',
    'log_every': 1e3,
    'train_every': 10,
    'prefill': 1e5,
    'actor_ent': 3e-3,
    'loss_scales.kl': 1.0,
    'discount': 0.99,
}).parse_flags()

import gym
import gym_minigrid

env = gym.make('MiniGrid-DoorKey-6x6-v0')
env = gym_minigrid.wrappers.RGBImgPartialObsWrapper(env)
train(env, config)







